{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExeciseHiLabML1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HydroinformaticsLab/ColabWorkshop/blob/master/ExeciseHiLabML1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SB93Ge748VQs"
      },
      "source": [
        "##### Gerald Augusto Corzo \n",
        "Practice Colab, IHE Delft 2020 - 05- 25\n",
        "Session 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEYuO5NFwDK9"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Reding data from rainfall\n",
        "\n",
        "1. Using google earth engine\n",
        "2. From your own google drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyZ6I6WGoCAx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1d79vtB9U2tsbCLdKom7oU_Hr1WhkPbqs#scrollTo=uyZ6I6WGoCAx\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/HydroinformaticsLab/ColabWorkshop/blob/master/ExeciseHiLabML1.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56V5oun18ZdZ"
      },
      "source": [
        "During the last years an increase on the number of machine learning algorithms libraries have boosted the creation of new ML cloud platforms. \n",
        "\n",
        "The development of new theories and new areas of science around machine learning motivates have motivated many researchers and large companies, google created \"colab.research.google.com\" \n",
        "\n",
        "This workshop aimts at providing basic experiences on the machine learning algorithms mentioned during the Hydroinformatics class at IHE Delft. \n",
        "\n",
        "The theory will not be covered since this was part of the DDM lectures provided. Aside, some links to google sites with tutorials and introductory videos will be shared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYLj6rDJr3eZ",
        "colab_type": "text"
      },
      "source": [
        "#Step 1 : Seeting up the environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDfL8DdysNp4",
        "colab_type": "text"
      },
      "source": [
        "## Authenticating google colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gk6JRhNrxrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7azbL8yEsLRm",
        "colab_type": "text"
      },
      "source": [
        "## Authenticating google earth engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgN9TqVqsLD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcS8UT2Ev39d",
        "colab_type": "text"
      },
      "source": [
        "## Load tensorflow libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYVeqeuMsK_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9430b4d-a293-4974-d531-810c56e0b5c0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6B95Hb6YVgPZ",
        "outputId": "2993d245-1643-4532-e80f-b43abd4381b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import folium\n",
        "print(folium.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5AgsPye61Q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "741afe2d-d85e-430d-ca6b-aea1c32ed358"
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 50%; color: blue;'><b>Welcome to Hydroinformatics Lab session</b></marquee>"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 50%; color: blue;'><b>Welcome to Hydroinformatics Lab session</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lkqTXIexKwq",
        "colab_type": "text"
      },
      "source": [
        "For this exercise we will select precipitation data. Google earth provide a large variety of data from different sources. Lest select daily aggregates from Era5 <https://developers.google.com/earth-engine/datasets/tags/precipitation>. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-nelXOvzbR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "region = ee.Geometry.Rectangle([79, 25, 89, 31])\n",
        "\n",
        "col = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
        "    .filterDate('1979-01-02', '1979-01-03')\\\n",
        "    .filterBounds(region)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_wqSAZExy6xV",
        "colab": {}
      },
      "source": [
        "# Your Earth Engine username.  This is used to import a classified image\n",
        "# into your Earth Engine assets folder.\n",
        "USER_NAME = 'username'\n",
        "\n",
        "# Cloud Storage bucket into which training, testing and prediction \n",
        "# datasets will be written.  You must be able to write into this bucket.\n",
        "OUTPUT_BUCKET = 'HI store'\n",
        "\n",
        "PEra5=ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
        "BandPEra = ['total_precipitation', 'maximum_2m_air_temperature', 'B4', 'B5', 'B6', 'B7']\n",
        "\n",
        "# Use Landsat 8 surface reflectance data for predictors.\n",
        "L8SR = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "# Use these bands for prediction.\n",
        "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SQp9VmnymMg",
        "colab_type": "text"
      },
      "source": [
        "To map the region where the rain is falling we are going to select the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ao7fJW1Pyiza",
        "colab": {}
      },
      "source": [
        "\n",
        "# This is a trianing/testing dataset of points with known land cover labels.\n",
        "LABEL_DATA = ee.FeatureCollection('projects/google/demo_landcover_labels')\n",
        "# The labels, consecutive integer indices starting from zero, are stored in\n",
        "# this property, set on each point.\n",
        "LABEL = 'landcover'\n",
        "# Number of label values, i.e. number of classes in the classification.\n",
        "N_CLASSES = 3\n",
        "\n",
        "# These names are used to specify properties in the export of\n",
        "# training/testing data and to define the mapping between names and data\n",
        "# when reading into TensorFlow datasets.\n",
        "FEATURE_NAMES = list(BANDS)\n",
        "FEATURE_NAMES.append(LABEL)\n",
        "\n",
        "# File names for the training and testing datasets.  These TFRecord files\n",
        "# will be exported from Earth Engine into the Cloud Storage bucket.\n",
        "TRAIN_FILE_PREFIX = 'Training_demo'\n",
        "TEST_FILE_PREFIX = 'Testing_demo'\n",
        "file_extension = '.tfrecord.gz'\n",
        "TRAIN_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
        "TEST_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
        "\n",
        "# File name for the prediction (image) dataset.  The trained model will read\n",
        "# this dataset and make predictions in each pixel.\n",
        "IMAGE_FILE_PREFIX = 'Image_pixel_demo_'\n",
        "\n",
        "# The output path for the classified image (i.e. predictions) TFRecord file.\n",
        "OUTPUT_IMAGE_FILE = 'gs://' + OUTPUT_BUCKET + '/Classified_pixel_demo.TFRecord'\n",
        "# Export imagery in this region.\n",
        "EXPORT_REGION = ee.Geometry.Rectangle([-122.7, 37.3, -121.8, 38.00])\n",
        "# The name of the Earth Engine asset to be created by importing\n",
        "# the classified image from the TFRecord file in Cloud Storage.\n",
        "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/Classified_pixel_demo'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z5pr9vuHVgXY"
      },
      "source": [
        "Using the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset as the example, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j-DHsby18cot",
        "outputId": "cb01be98-9c0e-43aa-e486-a39bb9635b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XKUjdIoV87um"
      },
      "source": [
        "## Using TensorBoard with Keras Model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CL_lxdn8-Sv"
      },
      "source": [
        "When training with Keras's [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit), adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with `histogram_freq=1` (this is off by default)\n",
        "\n",
        "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WAQThq539CEJ",
        "outputId": "c03f602e-6f2a-4642-bf4f-19c42f60a116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2194 - accuracy: 0.9356 - val_loss: 0.1034 - val_accuracy: 0.9679\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0987 - accuracy: 0.9699 - val_loss: 0.0847 - val_accuracy: 0.9739\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0691 - accuracy: 0.9787 - val_loss: 0.0711 - val_accuracy: 0.9783\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0535 - accuracy: 0.9829 - val_loss: 0.0732 - val_accuracy: 0.9766\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0433 - accuracy: 0.9857 - val_loss: 0.0681 - val_accuracy: 0.9806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc390f709e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nB718NOH95yG"
      },
      "source": [
        "## Using TensorBoard with other methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IKNt0nWs-Ekt"
      },
      "source": [
        "When training with methods such as [`tf.GradientTape()`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), use `tf.summary` to log the required information.\n",
        "\n",
        "Use the same dataset as above, but convert it to `tf.data.Dataset` to take advantage of batching capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nnHx4DsMezy1",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
        "test_dataset = test_dataset.batch(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SzpmTmJafJ10"
      },
      "source": [
        "The training code follows the [advanced quickstart](https://www.tensorflow.org/tutorials/quickstart/advanced) tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H2Y5-aPbAANs",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKhIIDj9Hbfy"
      },
      "source": [
        "Create stateful metrics that can be used to accumulate values during training and logged at any point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jD0tEWrgH0TL",
        "colab": {}
      },
      "source": [
        "# Define our metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "szw_KrgOg-OT"
      },
      "source": [
        "Define the training and test functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TTWcJO35IJgK",
        "colab": {}
      },
      "source": [
        "def train_step(model, optimizer, x_train, y_train):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(x_train, training=True)\n",
        "    loss = loss_object(y_train, predictions)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(y_train, predictions)\n",
        "\n",
        "def test_step(model, x_test, y_test):\n",
        "  predictions = model(x_test)\n",
        "  loss = loss_object(y_test, predictions)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_accuracy(y_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nucPZBKPJR3A"
      },
      "source": [
        "Set up summary writers to write the summaries to disk in a different logs directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Qp-exmbWf4w",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qgUJgDdKWUKF"
      },
      "source": [
        "Start training. Use `tf.summary.scalar()` to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other `tf.summary` functions enable logging other types of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odWvHPpKJvb_",
        "outputId": "cff49dc7-2c87-4ea0-f36f-f0af948d7a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = create_model() # reset our model\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for (x_train, y_train) in train_dataset:\n",
        "    train_step(model, optimizer, x_train, y_train)\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "  for (x_test, y_test) in test_dataset:\n",
        "    test_step(model, x_test, y_test)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print (template.format(epoch+1,\n",
        "                         train_loss.result(), \n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(), \n",
        "                         test_accuracy.result()*100))\n",
        "\n",
        "  # Reset metrics every epoch\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.24321186542510986, Accuracy: 92.84333801269531, Test Loss: 0.13006582856178284, Test Accuracy: 95.9000015258789\n",
            "Epoch 2, Loss: 0.10446818172931671, Accuracy: 96.84833526611328, Test Loss: 0.08867532759904861, Test Accuracy: 97.1199951171875\n",
            "Epoch 3, Loss: 0.07096975296735764, Accuracy: 97.80166625976562, Test Loss: 0.07875105738639832, Test Accuracy: 97.48999786376953\n",
            "Epoch 4, Loss: 0.05380449816584587, Accuracy: 98.34166717529297, Test Loss: 0.07712937891483307, Test Accuracy: 97.56999969482422\n",
            "Epoch 5, Loss: 0.041443776339292526, Accuracy: 98.71833038330078, Test Loss: 0.07514958828687668, Test Accuracy: 97.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JikosQ84fzcA"
      },
      "source": [
        "Open TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "0sK8X2O9bTlz",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}